* Sat 25 Mar 2023 08:25:40 PM CET
First conversation with ChatGPT on the project.
** Write a step by step guide to create a most basic devops project to show in portfolio. Fully automated, using most popular technologies. It is just a simplest pipeline in jenkins or circleci. It should be deployed by cloning a git repo and running terraform. Before you start, ask questions about details with possible options and arguments for each choice. Only generate the step by step guide after you have all required data.
*** Before creating the step-by-step guide for your basic DevOps project, I need some clarification on a few details. Please provide information on the following:

    Programming language: Which programming language is used for the application you want to build?
    Application type: Is it a web application, a command-line tool, or something else?
    Source code repository: Which Git hosting platform do you prefer (e.g., GitHub, GitLab, Bitbucket)?
    Infrastructure provider: On which cloud provider or platform do you want to deploy your application (e.g., AWS, GCP, Azure, DigitalOcean)?
    Target environment: Do you want to deploy to a specific environment like Kubernetes, or just to a virtual machine or container?

Once I have this information, I will provide you with a tailored
step-by-step guide for creating a basic DevOps project for your
portfolio.
** A.D. 1. I don't know, I would like something simple. In a later project version, we can use something more complicated. Please give me some options for a most basic project. Maybe some open source web app?
A.D. 2. I would like it to be a web app, that we would also deploy to
a web server using terraform and ansible/bash.
A.D. 3. GitHub,
A.D. 4. AWS A.D. 5. Virtual machine for CI/CD and container/VM for
webapp.

Remember it's a CV project. It should ask the person running it for
all credentials required to deploy to AWS, or take it from some
credentials file.
*** Thank you for providing the necessary details. Based on your preferences, I suggest using a simple Python Flask web application as the base project for your portfolio. Here's the step-by-step guide for creating a basic DevOps project:

    Fork a simple Python Flask web application:
    Go to https://github.com/CoreyMSchafer/code_snippets/tree/master/Python/Flask_Blog and fork the repository to your GitHub account.

    Clone the forked repository to your local machine:

    bash

git clone https://github.com/<your_github_username>/code_snippets.git
cd code_snippets/Python/Flask_Blog

Create a virtual environment and install dependencies:

bash

python -m venv venv
source venv/bin/activate
pip install -r requirements.txt

Test the application locally:

arduino

export FLASK_APP=flaskblog.py
flask run

Open your browser and visit http://127.0.0.1:5000 to view the web application.

Set up a Jenkins instance on AWS using Terraform and Ansible:

    Create a jenkins_setup directory and change into it:

    bash

        mkdir jenkins_setup
        cd jenkins_setup

        Create a main.tf file for Terraform configuration and add the necessary AWS resources (VPC, subnet, security group, and EC2 instance). Use the Amazon Linux 2 AMI and install Jenkins on the EC2 instance using user_data.
        Create an Ansible playbook (jenkins_playbook.yml) to install Jenkins and its dependencies, like Java and Git, on the EC2 instance.
        Set up AWS credentials by providing your access key and secret key either through environment variables or the ~/.aws/credentials file.
        Run terraform init to initialize the Terraform working directory.
        Run terraform apply to create the infrastructure on AWS.
        Once the EC2 instance is up and running, use ansible-playbook to run the jenkins_playbook.yml file.

    Configure Jenkins:
        Access the Jenkins instance using the public IP address of the created EC2 instance.
        Unlock Jenkins using the initial admin password found in /var/lib/jenkins/secrets/initialAdminPassword.
        Install the recommended plugins and set up an admin user.

    Create a Jenkins pipeline:
        In Jenkins, create a new pipeline job.
        In the pipeline configuration, select "Pipeline script from SCM" and provide the GitHub repository URL and credentials.
        Add a Jenkinsfile to the root of the Flask web application repository with the following stages:
            Checkout: Clone the repository.
            Build: Set up a Python virtual environment and install dependencies.
            Test: Run unit tests, if any.
            Deploy: Use a Dockerfile to containerize the application, push the Docker image to a container registry, and deploy the application to a container platform like Amazon ECS or a VM using Ansible.

    Trigger the Jenkins pipeline:
        Manually trigger the pipeline by clicking "Build Now" in Jenkins.
        Optionally, set up a webhook on GitHub to automatically trigger the pipeline whenever new changes are pushed to the repository.

    Access the deployed web application using the URL or public IP address of the target environment (e.g., Amazon ECS or the VM).

Now you have a fully automated DevOps project that showcases your
ability to create a CI/CD pipeline using popular technologies like
Jenkins, Terraform, and Ansible.
** The deployment of the CI/CD should be fully automated, configuring Jenkins by hand wont do. Maybe you have to use CircleCI to be able to have CI/CD configuration in YAML format?
*** You're correct. CircleCI allows you to store your CI/CD configuration in a YAML file, which simplifies the automation process. Here's the updated step-by-step guide for creating a basic DevOps project using CircleCI, Terraform, and Ansible:

    Fork the simple Python Flask web application:
    Follow the same steps as mentioned earlier to fork and clone the Flask web application.

    Sign up for CircleCI and connect your GitHub account:
        Go to https://circleci.com/signup/ and sign up using your GitHub account.
        Once logged in, add your forked Flask web application repository by clicking on "Add Projects" and selecting your repository.

    Create a CircleCI configuration file in your repository:
    In the root of the Flask web application repository, create a .circleci directory, and inside it, create a config.yml file with the following configuration:

#+BEGIN_SRC yaml

version: 2.1
jobs:
  build:
    docker:
      - image: circleci/python:3.8
    steps:
      - checkout
      - run:
          name: Set up virtual environment
          command: |
            python -m venv venv
            . venv/bin/activate
      - run:
          name: Install dependencies
          command: |
            . venv/bin/activate
            pip install -r requirements.txt
  deploy:
    docker:
      - image: circleci/python:3.8
    steps:
      - checkout
      - run:
          name: Set up virtual environment
          command: |
            python -m venv venv
            . venv/bin/activate
      - run:
          name: Install dependencies
          command: |
            . venv/bin/activate
            pip install -r requirements.txt
      - setup_remote_docker
      - run:
          name: Build and push Docker image
          command: |
            echo "$DOCKER_PASSWORD" | docker login -u "$DOCKER_USERNAME" --password-stdin
            docker build -t <your_dockerhub_username>/flask-webapp:latest .
            docker push <your_dockerhub_username>/flask-webapp:latest
      - run:
          name: Deploy to AWS
          command: |
            . venv/bin/activate
            ansible-playbook -i ansible/inventory.ini ansible/deploy.yml
workflows:
  version: 2
  build-and-deploy:
    jobs:
      - build
      - deploy:
          requires:
            - build
          filters:
            branches:
              only: main
#+END_SRC


Set up environment variables in CircleCI:

    Go to your project's settings in CircleCI.
    Under "Environment Variables", add the following variables:
        AWS_ACCESS_KEY_ID: Your AWS access key.
        AWS_SECRET_ACCESS_KEY: Your AWS secret key.
        DOCKER_USERNAME: Your Docker Hub username.
        DOCKER_PASSWORD: Your Docker Hub password.

Create an Ansible playbook for deployment:

    In the repository, create an ansible directory.
    Inside the ansible directory, create the following files:
        inventory.ini: A file containing the target AWS EC2 instance for deployment.
        deploy.yml: The main Ansible playbook to deploy the Docker container to the target instance.

The deploy.yml file should include tasks for:

    Installing Docker on the target instance.
    Logging in to Docker Hub.
    Pulling the latest Docker image of your Flask web application.
    Running a Docker container with the pulled image.

Set up AWS infrastructure using Terraform:
Follow the same
** Is it possible to make it even simpler? CircleCI requires logging in. Maybe it's possible to have jenkins configuration saved in a file and have jenkins load this configuration from repo? Maybe use a separate repo for the jenkins provisioning and installation and configuration?
*** I apologize for the confusion. Here's the complete step-by-step guide for creating a basic DevOps project using Jenkins Configuration as Code (JCasC), Terraform, and Ansible:

    Fork the simple Python Flask web application:
        Go to https://github.com/CoreyMSchafer/code_snippets/tree/master/Python/Flask_Blog and fork the repository to your GitHub account.
        Clone the forked repository to your local machine:

        bash

    git clone https://github.com/<your_github_username>/code_snippets.git
    cd code_snippets/Python/Flask_Blog

Create a separate repository for Jenkins provisioning, installation, and configuration:

    On GitHub, create a new repository called jenkins_config.
    Clone the new repository on your local machine:

    bash

    git clone https://github.com/<your_github_username>/jenkins_config.git
    cd jenkins_config

Set up Jenkins on AWS using Terraform and Ansible:

    Create a jenkins_setup directory and change into it:

    bash

mkdir jenkins_setup
cd jenkins_setup

Create a main.tf file for Terraform configuration and add the necessary AWS resources (VPC, subnet, security group, and EC2 instance). Use the Amazon Linux 2 AMI and install Jenkins on the EC2 instance using user_data.
Create an Ansible playbook (jenkins_playbook.yml) to install Jenkins and its dependencies, like Java, Git, and the JCasC plugin, on the EC2 instance:

yaml

    - name: Install Jenkins and dependencies
      hosts: all
      become: yes
      vars:
        jenkins_plugins:
          - name: git
          - name: configuration-as-code
      tasks:
        - name: Install Java
          package:
            name: java-1.8.0-openjdk-devel
            state: present

        - name: Add Jenkins repository
          yum_repository:
            name: jenkins
            description: Jenkins Repository
            baseurl: http://pkg.jenkins.io/redhat/jenkins.repo
            gpgkey: https://pkg.jenkins.io/redhat/jenkins.io.key
            gpgcheck: yes

        - name: Install Jenkins
          package:
            name: jenkins
            state: present

        - name: Start and enable Jenkins service
          systemd:
            name: jenkins
            state: started
            enabled: yes

        - name: Install Jenkins plugins
          jenkins_plugin:
            name: "{{ item.name }}"
          with_items: "{{ jenkins_plugins }}"

    Set up AWS credentials by providing your access key and secret key either through environment variables or the ~/.aws/credentials file.
    Run terraform init to initialize the Terraform working directory.
    Run terraform apply to create the infrastructure on AWS.
    Once the EC2 instance is up and running, use ansible-playbook to run the jenkins_playbook.yml file.

Configure Jenkins using JCasC:

    In the jenkins_config repository, create a jenkins.yaml file for Jenkins Configuration as Code (JCasC).
    Add the following configuration to create a pipeline job:

    yaml

jobs:
  - script: >
      pipelineJob('flask-webapp') {
        definition {
          cpsScm {
            scm {
              git {
                remote { url('https://github.com/<your_github_username>/code_snippets.git') }
                branches('master')
              }
            }
            scriptPath
** How could the Jenkins provisioning (`jenkins_playbook.yml`) playbook be modified to also install the Jenkins config from the `jenkins.yml` file?
*** To modify the jenkins_playbook.yml to install the Jenkins config from the jenkins.yaml file, you can add a few tasks to the Ansible playbook. These tasks will clone the jenkins_config repository, copy the jenkins.yaml file to the JCasC configuration directory, and restart Jenkins to apply the changes.

Here's the updated jenkins_playbook.yml:

yaml

- name: Install Jenkins and dependencies
  hosts: all
  become: yes
  vars:
    jenkins_plugins:
      - name: git
      - name: configuration-as-code
  tasks:
    - name: Install Java
      package:
        name: java-1.8.0-openjdk-devel
        state: present

    - name: Add Jenkins repository
      yum_repository:
        name: jenkins
        description: Jenkins Repository
        baseurl: http://pkg.jenkins.io/redhat/jenkins.repo
        gpgkey: https://pkg.jenkins.io/redhat/jenkins.io.key
        gpgcheck: yes

    - name: Install Jenkins
      package:
        name: jenkins
        state: present

    - name: Start and enable Jenkins service
      systemd:
        name: jenkins
        state: started
        enabled: yes

    - name: Install Jenkins plugins
      jenkins_plugin:
        name: "{{ item.name }}"
      with_items: "{{ jenkins_plugins }}"

    - name: Clone Jenkins configuration repository
      git:
        repo: https://github.com/<your_github_username>/jenkins_config.git
        dest: /var/lib/jenkins/jenkins_config

    - name: Ensure JCasC configuration directory exists
      file:
        path: /var/lib/jenkins/configuration-as-code
        state: directory
        owner: jenkins
        group: jenkins
        mode: '0755'

    - name: Copy Jenkins configuration file
      copy:
        src: /var/lib/jenkins/jenkins_config/jenkins.yaml
        dest: /var/lib/jenkins/configuration-as-code/jenkins.yaml
        owner: jenkins
        group: jenkins
        mode: '0644'

    - name: Restart Jenkins to apply configuration changes
      systemd:
        name: jenkins
        state: restarted

These new tasks perform the following actions:

    Clone the jenkins_config repository to /var/lib/jenkins/jenkins_config on the Jenkins EC2 instance.
    Ensure that the JCasC configuration directory /var/lib/jenkins/configuration-as-code exists.
    Copy the jenkins.yaml file from the cloned repository to the JCasC configuration directory.
    Restart Jenkins to apply the configuration changes.

After running the updated playbook, Jenkins will load the
configuration from the jenkins.yaml file located in the JCasC
configuration directory.
* Sun 26 Mar 2023 09:30:15 AM CEST
** Preparing org-style plan for what needs to be done.
Offtop: My eyes hurt a little and vision is blurry again, think it's because some of the under-eyes cream got into the eyes.
*** It's easier to follow the instructions in ChatGPT web app than what's copied into the org document. Might be because the numbered list doesn't get copied correctly (it's copied without the numbers).
*** Going to use the basic 6 steps from the prompt as the first level tasks for the org document.
*** For now leaving it with just 6 steps, will plan each step as I approach it.

** Copying the code snippets from ChatGPT into one organized place.
*** Code is already in this org file. I've made screenshots of the whole conversation in case I need some more organized reference.
** Moved the journal to another file, so it can be removed from the repo if it's required.
At first thought to remove it completely from git tracking. Then I
would lose the advantage of having notes and thoughts connected with
specific code changes in one place.

After thinking deeper, I realized that I wanted to hide the fact that
I used ChatGPT to help me decide on what project to choose for MVP
DevOps project for most basic portfolio of one. But after considering
it, I've realized that the only thing that would be dishonest here
would be trying to hide that I got help from a ChatGPT tutor.

** A problem needs solving before I start work. This project calls for usage of two more repositories, while I like to have one big repo that also includes the journal files. Maybe if I could explain the situation verbosely enough to ChatGPT, it could have some insights. I'm using the same conversation as for the whole project, so I assume that ChatGPT has context enough for this question (i.e. I don't have to tell it about the project).
You are a tutor, Senior DevOps expert helping me, aiming for a Junior
DevOps position, by giving me guidance and assistance where I need it.


I like to work by writing a project journal with general thoughts and
notes on what I'm doing and why I'm doing it, and also a project
documentation with more specific information on steps performed when
working on a project with details on each step so it can be understood
later on. Because of that I tended to have one big repository for both
the code and journals, so commits with code also include journal
commits, which gives each code commit much more context.

This project however requires usage of two more repositories. One for
Jenkins deployment and Configuration. Second one being the cloned
repository of the flask web app.

Later web app maintenance is not a concern. Readability is, but even
more important is to not make the project too complicated, so I can
finish it quickly. It's just a portfolio project to showcase basic
DevOps skills, so I can have something to link from the CV.

What should I do?
I see three options, but maybe you have other insights:

Option one: Create just one big repo, with the two other repos not
really repos but directories inside the big repo. It will make work on
the 'Jenkins deployment and configuration' part much easier, as each
code commit will also be accompanied by relevant journal entries. It
will also make it near-impossible to have CI/CD pipeline triggered on
new commits, since the commits will be not only to the cloned flask
app, but also to CI/CD configuration and deployment.

Option two: Create one big repo, and then somehow create two others
inside it. I'm not sure if it's possible with git. It might cause
problems later on, but as long as basic deployment of the CI/CD
pipeline is possible, it should be ok. However, I'm not shy of sharing
these journals publicly.

Option three: Create three separate repos in three separate directory
trees. One for journals and files specific to the whole project,
second for CI/CD deployment and configuration, third for the web
app. This means that repos will not disturb each other, but also that
the journal entries wont be connected with code commits, so I can't
show my thought process and can't easily see documentation entries
connected to code entries.

Option four (idea I had while writing down previous options): Have two
repos. First one would merge the repo for the project as a whole and
the CI/CD pipeline. Showcasing CI/CD skills is at the heart of the
project. The second repo with the cloned web app repository could
include just the web app (and eventually some CI/CD configuration that
would normally reside in the same repo as the app). Then commits to
this separate repo could properly trigger build and deployment process
from Jenkins.

*** Based on your requirements and priorities, I suggest going with Option Four. This option strikes a balance between simplicity and showcasing your DevOps skills, while also allowing you to maintain your project journal alongside the code.

Here's a breakdown of Option Four:

    Create a main repository for the project documentation, journal, and CI/CD pipeline configuration. This repository will include the following directories and files:
        /project_journal - for your project journal and general thoughts.
        /project_documentation - for your project documentation with step-by-step details.
        /jenkins_config - for Jenkins provisioning, installation, and configuration.
        /terraform - for your Terraform configuration files.
        /ansible - for your Ansible playbooks.

    Fork the Flask web application repository and clone it. This will serve as your second repository. You can add the required CI/CD configuration files (such as Jenkinsfile) directly to this repository. This will allow Jenkins to trigger builds and deployments based on commits to this repository.

This way, you can have your project journal and documentation connected to the CI/CD pipeline configuration and deployment, while keeping the web application repository separate. The separation ensures that your CI/CD pipeline works as expected, and it demonstrates your ability to create and manage a basic DevOps project for your portfolio.

Remember to update the paths in your CI/CD pipeline configuration and
scripts to point to the correct directories in your new repository
structure.

*** Comment: Just describing the problem well let me have an insight on how to solve it. I wonder if some of the power of large language models doesn't lie in the rubberducking nature of using them. Rubber-ducking is a process of troubleshooting a problem by first describing it thoroughly to a rubber duck before we go and ask other people about it. What's special about it, is that just by talking and explaining the problem to a rubber duck, or any imaginary entity, we often arrive at solutions to our problems.

* Sun 26 Mar 2023 11:57:06 AM CEST
** After deciding on how project will look (including repos and such), I'm preparing the first commit that will include basic directory structure, but also contain project's description. Actually I'm going to cut a corner here and have ChatGPT generate project's description.
*** Create a project description to be included at the beginning of project documentation file. It is to be read by people checking out the portfolio. You can base it on this:

This is a simple CI/CD project to showcase my skills in ....

It's aim is for someone to be able to download this repository and run it
himself to create a working CI/CD pipeline in AWS based on Jenkins
that deploys a container with a flask web app to AWS each time a
commit is made to the tracked flask app repository.
**** Title: Basic DevOps Portfolio Project - CI/CD Pipeline with Jenkins and AWS
Project Description:

Welcome to my Basic DevOps Portfolio Project. The primary goal of this project is to showcase my skills in setting up a Continuous Integration and Continuous Deployment (CI/CD) pipeline using Jenkins, AWS, and containerization.

In this project, you will find a fully functional CI/CD pipeline that automates the process of deploying a Flask web application to an AWS environment each time a commit is made to the tracked repository. The pipeline is designed to be easily reproducible, allowing you to download this repository and set up the pipeline in your own AWS account with minimal effort.

The project highlights my proficiency in:

    Jenkins: Configuring and managing Jenkins as a CI/CD tool, including setting up jobs, pipelines, and plugins.
    AWS: Leveraging various AWS services, such as EC2, RDS, and ECS, to create a scalable and secure infrastructure for the web application.
    Terraform: Using Terraform to define and provision the infrastructure resources required for the pipeline and the web application.
    Ansible/Bash: Automating the deployment and configuration of the Jenkins server, as well as the web application, using Ansible playbooks or Bash scripts.
    Containerization: Packaging the Flask web application in a Docker container to ensure consistent deployment across environments.

By following the detailed instructions provided in this project documentation, you will be able to create a working CI/CD pipeline in your own AWS account, gaining a deeper understanding of the processes and tools involved in modern DevOps practices.
**** Comment: Description generated by GPT-4 is flawless. Why I feel uneasy about it then? For a Junior engineer it's important to do things yourself, that's the only way to learn them. I guess in places that are not a core ability for a DevOps, cutting corners like that may be essential to work against my tendency to over complicate things and to fall into rabbit holes. On the other hand, if I'm doing something that I later want to say I can do as a professional, I might use ChatGPT as a web search or a guide on what to do next or how to solve a problem, but not to get a complete solution that I would mindlessly copy. Still, there is a certain knowledge required to use these solutions correctly and to spot when ChatGPT is hallucinating. So I shouldn't accredit all work to ChatGPT either.
* Sun 26 Mar 2023 02:57:37 PM CEST
** Just posting this project description would make me feel weak inside. How can I use it and still have clean conscience? By adding a note that it was written by ChatGPT and also informing potential readers that ChatGPT assisted in planning the project and writing parts of the documentation. Maybe just a general info in the project description, and then specific notes in places where ChatGPT does most of the work, like in the project description? This feels good. I have a feeling that this kind of ethical questions is what many people ask themselves at the moment. Problem in my case is that if I just had ChatGPT do 90% of the work in every aspect, it would be insincere to claim that I've done the work or that it shows any skills on my part other than prompt engineering.
** I should explicitly say in the project's readme (which can be just a .md version of project description from the documentation .org file) what ChatGPT helped with.
* Thu 06 Apr 2023 05:09:35 PM CEST
In the past 1.5 weeks I've put about 31 pomodoros into learning
Terraform. I think that's taking too much time, going to experiment
with a prompt to create a seasoned DevOps professional guide and ask
him whether I should keep following the Terraform Up and Running book
sequentially or just try to learn the absolute minimum required to
continue with the project.

I've learned a little more about using ChatGPT and practiced a
lot. I've also found a project that takes in your prompt and upgrades
it to be more effective. https://promptperfect.jina.ai/prompts
** Prompt: 
As an experienced DevOps guide and tutor, your task is to provide
advice for a user working on a basic portfolio project involving the
deployment of a Python Flask web application using Jenkins. The user
has spent 1.5 weeks learning Terraform but feels that it may be taking
too much time.

Please evaluate whether they should continue with the "Terraform Up
and Running" book sequentially or focus only on learning the minimum
required to complete their current project. In your response, consider
factors such as efficiency, long-term benefits, and skill development
in relation to a junior DevOps professional's career
growth. Additionally, feel free to offer alternative suggestions if
you believe there are more effective ways for them to achieve their
goals while also considering time constraints and overall
effectiveness in accomplishing their objectives.

Your response should clearly explain the pros and cons of each option
(continuing with the book versus focusing on the minimum
requirements), provide recommendations based on your evaluation, and
offer any alternative approaches you think might help them reach their
goals more effectively. Please ensure that your advice takes into
account both short-term needs related to completing the project at
hand and long-term career growth considerations for a junior DevOps
professional.

This is what can be found in the book, right now I'm halfway trough the second chapter.
Chapter 2, “Getting Started with Terraform”
Installing Terraform; an overview of Terraform syntax; an overview of
the Terraform CLI tool; how to deploy a single server; how to deploy a
web server; how to deploy a cluster of web servers; how to deploy a
load balancer; how to clean up resources you’ve created.
Chapter 3, “How to Manage Terraform State”
What Terraform state is; how to store state so that multiple team
members can access it; how to lock state files to prevent race
conditions; how to isolate state files to limit the damage from errors;
how to use Terraform workspaces; a best-practices file and folder layout
for Terraform projects; how to use read-only state.
Chapter 4, “How to Create Reusable Infrastructure with Terraform
Modules”
What modules are; how to create a basic module; how to make a
module configurable with inputs and outputs; local values; versionedmodules; module gotchas; using modules to define reusable,
configurable pieces of infrastructure.
Chapter 5, “Terraform Tips and Tricks: Loops, If-Statements, Deployment,
and Gotchas”
Loops with the count parameter, for_each and for expressions,
and the for string directive; conditionals with the count parameter,
for_each and for expressions, and the if string directive; built-in
functions; zero-downtime deployment; common Terraform gotchas and
pitfalls, including count and for_each limitations, zero-downtime
deployment gotchas, how valid plans can fail, and how to refactor
Terraform code safely.
** Reply: 
As an experienced DevOps guide and tutor, I understand the importance
of balancing short-term project goals with long-term career
development. In this case, the user has spent 1.5 weeks learning
Terraform and is considering whether to continue with the "Terraform
Up and Running" book sequentially or focus only on learning the
minimum required to complete their current project.

Option 1: Continue with the book sequentially Pros:

    Comprehensive understanding of Terraform's features and best
    practices.  Developing a strong foundation in Terraform, which is
    widely used in the industry.  Exposure to advanced Terraform
    concepts that can be useful for future projects.

Cons:

    Time-consuming, which may delay the completion of the current
    project.  Some topics covered in the book may not be immediately
    relevant to the user's current project.

Option 2: Focus on the minimum requirements Pros:

    Faster completion of the current project.  Allows the user to
    focus on learning just the necessary Terraform concepts for the
    project.

Cons:

    Limited understanding of Terraform's features and best practices.
    May need to revisit the book in the future for more advanced
    topics.

Recommendation: Considering the user's current situation, I recommend
focusing on the minimum requirements to complete the project. This
approach will allow the user to apply their Terraform knowledge more
quickly, demonstrate their skills in a portfolio project, and
potentially secure a junior DevOps position sooner.

To ensure the user's long-term career growth, I suggest the following
alternative approach:

    Finish Chapter 2 of the book, which covers the basics of Terraform
    and should be sufficient for the current project.  Prioritize
    applying the learned concepts in the current project, focusing on
    building a functional CI/CD pipeline for the Flask web
    application.  After completing the project, return to the book and
    continue learning more advanced Terraform concepts (Chapters 3-5)
    at a comfortable pace. This way, the user can gradually build
    their Terraform expertise without delaying the current project.

This approach balances the user's short-term project goals with their
long-term career development, ensuring they have the necessary skills
to complete their project while also laying the groundwork for future
growth as a DevOps professional.

** Conclusions: I'm finishing the second chapter, focusing especially on what might be relevant to the flask CI/CD pipeline project.

* Sun 16 Apr 2023 12:45:42 PM CEST
Had a first sit down to project after I've spent some time learning
Terraform. Downloaded the code for the flask app that will be later
committed to the code repo of the project (infrastructure and CI/CD
part has it's own repo, that also includes documentation and the
journal). I'm going to make one last jump into Terraform Up and
Running to learn about data sources. This will provide a solid
foundation of Terraform knowledge, sufficient for this project
and a basis for building further Terraform competence in the future.

* Tue 18 Apr 2023 06:45:06 PM CEST
With both the pipeline and code repositories ready, time to create a
VM in the cloud to hold the Jenkins server. But what OS to use?
** Choose OS and AMI
*** Debian or Redhat family?
I used to do servers using CentOS. Since CentOS got nuked, I was
wondering whether Fedora wouldn't be a good choice for this
project. Another alternative would be Alma Linux, which is a RHEL
repackaged system in CentOS spirit. Another similar to Alma is Rocky
Linux. I don't exclude Debian family from the equation, especially if
there are some issues standing Jenkins on Redhat family.

I think I'll go with Alma, because I haven't tried it before and it's
kind of CentOS, so I might use it in the future for similar use
cases. Fedora is on second place. If there are any repos missing for
Alma that exist for Fedora, I'll fallback to Fedora Server edition.
*** AMI, use an existing or build own?
I could build my own ami using Packer, but to reduce complexity, I'll
go with a ready-made AMI from AWS AMI Marketplace.
* Wed 19 Apr 2023 08:38:35 PM CEST
Trying to iron out the details for the Terraform setup. It's quite
simple, because it's just one instance. There are however some aspects
that were not covered in the part of Terraform Up and Running I've
read. I need to make sure the Instance gets my ssh-key baked in. The
firewall rules (security group) also has to accept Jenkins traffic.
* Sat 22 Apr 2023 01:47:05 PM CEST
Preparing what's required for the Terraform configuration. In the
simplest form, I just need a Linux cloud instance with a web and ssh
ports open. I also need to pass the IP address of the instance to
Ansible.

Firewall configuration will be done using AWS Security Group. SSH key
will be generated on host and inserted onto the instance using
Terraform. I'll probably just get the instances IP using Terraform
output variable and paste it into the inventory file with a script.

** Getting the AMI
AMI IDs are a source of fun (problems), because the same virtual
machine image will have different AMI number on each region.

I've downloaded AMI IDs list from Alma Linux project's website, file
is named aws_amis.csv. Need to extract the correct (and just one) AMI
number from it. Or find an alternative that doesn't include
downloading files from Alma's website.
*** grep method
$ grep eu-north-1 aws_amis.csv | grep x86 | sort -nrk 2 -t',' | head -1 | cut -f4 -d','
*** awk method
$ awk -F, '$3 == "\"eu-north-1\"" && $5 == "\"x86_64\"" { \               
           version=$2; \
           if (version > max_version) { \
             max_version = version; \
             ami_id = $4 \
           } \
         } \
         END { print ami_id }' aws_amis.csv
*** Terraform Data Source method
Still didn't like downloading the file (should it be in repo, should
it be re-downloaded each time?) and grepping for the AMI. Had a hunch
that it might be solved in Terraform, so I asked if it's possible to
have an elegant solution using Terraform only:

To use Terraform Data Source to get the AMI of AlmaLinux OS 9 for x86_64 on the `eu-north-1` region, you can use the `aws_ami` data source with the appropriate filters. Here's a Terraform configuration that demonstrates this:

#+BEGIN_SRC hcl
provider "aws" {
  region = "eu-north-1"
}

data "aws_ami" "alma_linux_9" {
  most_recent = true

  filter {
    name   = "name"
    values = ["AlmaLinux OS 9*"]
  }

  filter {
    name   = "architecture"
    values = ["x86_64"]
  }

  filter {
    name   = "root-device-type"
    values = ["ebs"]
  }

  filter {
    name   = "virtualization-type"
    values = ["hvm"]
  }

  filter {
    name   = "hypervisor"
    values = ["xen"]
  }

  owners = ["amazon", "aws-marketplace", "self"]
}

output "alma_linux_9_ami_id" {
  value = data.aws_ami.alma_linux_9.id
}
#+END_SRC


This configuration sets up the AWS provider for the `eu-north-1` region and defines an `aws_ami` data source that filters the available AMIs based on the name pattern, architecture, and other properties. The `most_recent = true` option ensures that you get the latest available version of the AMI.

After running `terraform init` and `terraform apply`, the output should display the AMI ID of the most recent AlmaLinux OS 9 for x86_64 in the `eu-north-1` region.

* Mon 01 May 2023 02:40:31 PM CEST
Back after a break. From the three methods to get AMI, the terraform
looks like easiest to read and maintain. AWK is cool, but few people
know it well. And it depends on the Alma site working and not changing
the name of the file with list of the AMIs. Terraform solution needs
the AWS to be online, just like the rest of the project.

I've copied the main.tf file from what I wrote with Terraform Up and
Running. Need to slightly modify it before I can commit both the AMI
data source and the initial version of the main.tf.

I've also got an example file from ChatGPT. Had it explain the file
line by line. There are few differences between the file I had
already, but ChatGPT made good arguments on why the changes were
introduced.

** Differences:
*** Defined VPC and Subnet
VPC and Subnet are a must have if our infrastructure is not the only
one defined on that account. It's a good practice to define them, as
it enhances security by isolating this and other projects. Subnets
allow for even more network segregation.

They also allow usage of more advanced network tools like Network ACL
(Access Control Lists) and Route Tables.
**** I'm keeping the definitions.
*** Explicitly defining egress traffic rules in the Security Group
I asked for explanation on why to do that, if the default AWS behavior
is to allow outbound traffic anyway. Reply was that it shows that all
outgoing traffic is allowed purposefully and also makes it easier to
understand our intentions for people reading our IaC.

* Sun 07 May 2023 01:09:21 PM CEST
Replaced variable code parts like region, instance type or ports with
variables, defined in the vars.tf. Going to commit the first version
of Terraform files before taking on the SSH keys.

It's not proper to make huge commits with multiple changes. I'm
dividing changes into smaller commits. Divided it into commits with:
a decision on what would make Terraform part of the project complete,
adding the data source that fetches the AMI of latest Alma Linux,
adding the two alternatives of main.tf file,
finally consolidating the two files into one with some variables
separated into a vars.tf file.

* Tue 09 May 2023 07:31:53 PM CEST
Time to solve the problem of SSH keys. I have no idea what are good
practices of SSH keys management when working with a project like
that.

Maybe I'll make a script that would auto generate such a key, put it
in a safe LUKS vault and also upload public key to AWS.

** Generated instructions on how to create and upload an ssh key:

I understand your concern about SSH key management. Here's an approach you can consider for your project:

    Generate a new ed25519 SSH key pair with a custom number of iterations for the key derivation function (KDF):

bash

ssh-keygen -t ed25519 -a <number_of_iterations> \
  -f <project_location>/my_project_key

Replace <number_of_iterations> with the desired number of iterations (e.g., 100) and <project_location> with the path to your project directory.

    Upload the generated public key to AWS using Terraform:

hcl

resource "aws_key_pair" "my_project_key" {
  key_name   = "my_project_key"
  public_key = file("<project_location>/my_project_key.pub")
}

    Use the generated key pair when creating the EC2 instance:

hcl

resource "aws_instance" "jenkins" {
  ami           = <ami_id>
  instance_type = "t2.micro"

  key_name = aws_key_pair.my_project_key.key_name

  # Other configurations ...
}

    Add the private key file (my_project_key) to your .gitignore file to avoid accidentally committing it to your version control system.

    Configure your Ansible inventory and playbook to use the generated private key when connecting to the remote host.

This approach will generate a project-specific SSH key pair in your
project directory, upload the public key to AWS, and associate it with
the EC2 instance. This allows you to manage the keys more efficiently
and avoid using a single key for multiple projects or machines.


** Generated instructions on how to create a LUKS container for SSH key:
To create a small LUKS container to store your SSH key securely at rest, follow these steps:

    Install the required packages for LUKS (Linux Unified Key Setup). On Debian/Ubuntu-based systems, you can use the following command:

bash

sudo apt-get install cryptsetup

On CentOS/RHEL/Fedora systems:

bash

sudo yum install cryptsetup

    Create an empty file to use as the LUKS container. Here, we'll create a 10MB container:

bash

dd if=/dev/zero of=my_ssh_key_container.img bs=1M count=10

    Set up the LUKS container using the cryptsetup command:

bash

sudo cryptsetup luksFormat my_ssh_key_container.img

This command will prompt you to enter a passphrase to protect the container. Make sure you choose a strong passphrase.

    Open the LUKS container:

bash

sudo cryptsetup luksOpen my_ssh_key_container.img my_ssh_key_container

Enter the passphrase you set earlier when prompted.

    Create a file system inside the container:

bash

sudo mkfs.ext4 /dev/mapper/my_ssh_key_container

    Mount the container to a directory:

bash

mkdir my_ssh_key_mount
sudo mount /dev/mapper/my_ssh_key_container my_ssh_key_mount

    Now you can copy the SSH key files into the mounted directory:

bash

sudo cp <project_location>/my_project_key my_ssh_key_mount/
sudo cp <project_location>/my_project_key.pub my_ssh_key_mount/

    Set the appropriate ownership and permissions for the key files:

bash

sudo chown $(whoami):$(whoami) my_ssh_key_mount/my_project_key*
sudo chmod 600 my_ssh_key_mount/my_project_key
sudo chmod 644 my_ssh_key_mount/my_project_key.pub

    When you're done using the keys, unmount the container and close it:

bash

sudo umount my_ssh_key_mount
sudo cryptsetup luksClose my_ssh_key_container

This will create a LUKS container, store your SSH keys securely inside
it, and allow you to mount and unmount the container as needed. To
access the keys, you'll need to open the container with the
passphrase, mount it, and then use the keys from the mounted
directory.

** Generated a script to create, open and close the container.


#+BEGIN_SRC bash
#!/bin/bash

set -e

VAULT_DIR="vault"
CONTAINER="$VAULT_DIR/my_ssh_key_container.img"
MOUNT_POINT="$VAULT_DIR/my_ssh_key_mount"

function create_container() {
  dd if=/dev/zero of="$CONTAINER" bs=1M count=30
  sudo cryptsetup luksFormat "$CONTAINER"
  sudo cryptsetup luksOpen "$CONTAINER" \
    my_ssh_key_container
  sudo mkfs.ext4 /dev/mapper/my_ssh_key_container
  sudo cryptsetup luksClose my_ssh_key_container
}

function open_container() {
  sudo cryptsetup luksOpen "$CONTAINER" \
    my_ssh_key_container
  sudo mkdir -p "$MOUNT_POINT"
  sudo mount /dev/mapper/my_ssh_key_container "$MOUNT_POINT"
}

function close_container() {
  sudo umount "$MOUNT_POINT"
  sudo cryptsetup luksClose my_ssh_key_container
}

case "$1" in
  create)
    create_container
    ;;
  open)
    open_container
    ;;
  close)
    close_container
    ;;
  *)
    echo "Usage: $0 {create|open|close}"
    exit 1
    ;;
esac
#+END_SRC

** V2 version of script for management of LUKS container

#+BEGIN_SRC bash
  # Here's the updated script that uses a SHELL_VARIABLE for the passphrase when creating and opening the LUKS container. The second script initializes the project and creates an SSH key inside the container.

  # vault_management.sh:

  # bash

  #!/bin/bash

  set -e

  VAULT_DIR="vault"
  CONTAINER="$VAULT_DIR/my_ssh_key_container.img"
  MOUNT_POINT="$VAULT_DIR/my_ssh_key_mount"
  PASS_VAR="VAULT_PASSPHRASE"

  function create_container() {
    dd if=/dev/zero of="$CONTAINER" bs=1M count=30
    echo -n "${!PASS_VAR}" | sudo cryptsetup luksFormat \
      --key-file=- "$CONTAINER"
    open_container
    sudo mkfs.ext4 /dev/mapper/my_ssh_key_container
    close_container
  }

  function open_container() {
    echo -n "${!PASS_VAR}" | sudo cryptsetup luksOpen \
      --key-file=- "$CONTAINER" my_ssh_key_container
    sudo mkdir -p "$MOUNT_POINT"
    sudo mount /dev/mapper/my_ssh_key_container "$MOUNT_POINT"
  }

  function close_container() {
    sudo umount "$MOUNT_POINT"
    sudo cryptsetup luksClose my_ssh_key_container
  }

  case "$1" in
    create)
      create_container
      ;;
    open)
      open_container
      ;;
    close)
      close_container
      ;;
    ,*)
      echo "Usage: $0 {create|open|close}"
      exit 1
      ;;
  esac
#+END_SRC

** Generated a script to init the project, create the ssh key and the container. Also auto export passphrase so it can be used in the project.


#+BEGIN_SRC bash
#!/bin/bash

set -e

VAULT_DIR="vault"
MOUNT_POINT="$VAULT_DIR/my_ssh_key_mount"
KEY_NAME="my_project_key"
PASS_VAR="VAULT_PASSPHRASE"

# Prompt for the passphrase
echo -n "Enter the passphrase for the LUKS container and SSH key: "
read -s PASSPHRASE
echo
export $PASS_VAR="$PASSPHRASE"

# Create and open the LUKS container
./vault_management.sh create
./vault_management.sh open

# Generate the SSH key inside the container
echo -n "${!PASS_VAR}" | ssh-keygen -t ed25519 \
  -N "" -f "$MOUNT_POINT/$KEY_NAME"

# Close the LUKS container
./vault_management.sh close
#+END_SRC

In this version, the vault_management.sh script uses the passphrase
stored in the SHELL_VARIABLE when creating and opening the LUKS
container. The init_project.sh script initializes the project, creates
the container, generates an SSH key inside it, and then closes the
container. The passphrase is read once using read -s and stored in the
SHELL_VARIABLE.

* Sat 13 May 2023 12:59:08 PM CEST
A script to create, open and close the LUKS container will simplify
usage of the vault for SSH key and possibly other secrets.

A script to glue the whole Jenkins deployment process would also be
useful. It can use the vault_management script to create, open vault
and generate an SSH key inside.

Initial vault management script is ready, but it shouldn't create a
container if there is one already.
* Sun 21 May 2023 03:33:45 PM CEST
General script for the project "execution", starting with
pseudocode. Eventually moving on to steps defined in bash. Expected
semi-production grade, but aiming at professional quality, easily
usable by non-experts.

Volume management and ssh creation automation can be a bit more rough.

* Tue 30 May 2023 08:26:54 PM CEST
Another short sit down to the project, hopefully can get momentum back
up after a break.

Finishing up the vault_management script.

* Tue Jun  6 02:50:54 PM CEST 2023
Some really annoying thing happened to date, where as above, the year
got moved to the end of the date string printed by vanilla `date`
command. Only on Fedora, happened when updating from Fedora 36 to 38.
Might consider binding `date +'%a %d %b %Y %X %Z'` as `date`, but for
now can just use it to generate a proper date. So annoying when things
are not correct.

* Tue 06 Jun 2023 02:53:54 PM CEST
I have a working container management script, but it doesn't detect if
container is already open and reports an error in this case. Fixing it
smells of Yak Shaving, so I'm leaving it as an extra TODO.

* Thu 08 Jun 2023 03:21:05 PM CEST
Starting pseudocode for the general project execution script. Would be
nice to find a better name for it.

For now it's called start_project.sh.

Wrote all the main points of the project, from creating a VM, through
provisioning a Jenkins instance and configuring it (setting up a CI/CD
pipeline) to testing Jenkins by pushing a commit to the app repository
and testing if a new container has been build and pushed out.
It will probably become more detailed as the project develops.

Because there is a DJ playing at the airport where I'm sitting now,
and the master script is going play trough all the points in the
script, I'm renaming it to playtrough.sh.

* Thu 08 Jun 2023 04:58:58 PM CEST
Defined some more steps in the playtrough.sh.

* Sat 10 Jun 2023 02:58:43 PM CEST
Consultation with a friend revealed that using a LUKS Vault for
secrets is not a typical approach and Hashicorp's Vault would be
better. LUKS Vault is fine for now though. But upgrading the
vault_management.sh is kind of pointless (if a similar amount of work
can be put into learning to use an industry standard tool).
* Sun 11 Jun 2023 12:21:44 PM CEST
For testing purposes (integration testing of the whole project and
it's elements), it would be cool to have a kind of fall trough switch
statement. So that we could (knowing first which number belongs to
which step in project's playtrough) execute only certain chain of
steps, maybe starting in the middle or not going all the way to the
end.

Bash doesn't have such a statement, but it can be simulated and even
improved by using a chain of if tests testing variables START_POINT
and LAST_POINT. So that for each step, the variables are tested and
they would prevent re-running non idempotent steps to avoid extra fun
(problems). Fully idempotent script is a goal, but it's lower priority
than having a working MVP of the project.

Example:
#+BEGIN_SRC bash
  START_POINT=2
  LAST_POINT=3

  STEP=1
  if [[ (( START_POINT <= STEP )) && (( LAST_POINT >= STEP )) ]]; then
    echo "step 1"
  fi
  STEP=2
  if [[ (( START_POINT <= STEP )) && (( LAST_POINT >= STEP )) ]]; then
    echo "step 2"
  fi
  STEP=3
  if [[ (( START_POINT <= STEP )) && (( LAST_POINT >= STEP )) ]]; then
    echo "step 3"
  fi
  STEP=4
  if [[ (( START_POINT <= STEP )) && (( LAST_POINT >= STEP )) ]]; then
    echo "step 4"
  fi
#+END_SRC

This would print:
step 2
step 3

Kinda cool, and copying and pasting can be used to save a lot of
writing.

* Tue Jun 13 11:54:45 AM CEST 2023
Got an idea that I can quickly find out if the current directory is
project's root dir by testing for the presence of a file. Same goes
for checking of vault accessibility.

To have a persistent vault password, it can be exported as a shell
variable. Exported shell variables are accessible from inside of
scripts run in the same shell, as proven with this script:

#+BEGIN_SRC bash
ENV_VAR_NAME="var1"

echo "This is the value of var ${ENV_VAR_NAME}:"
echo "${!ENV_VAR_NAME}"
exit 0
#+END_SRC

* Wed 14 Jun 2023 09:39:44 AM CEST
Just solved an annoying bug, where a test for password length would
fail (and say that password is empty) because I miss-typed the
variable name. I then copied the whole test few lines later in code
(to make sure that password was properly assigned), which made the
typo named variable VAULT_PASSHPRASE appear twice and not be detected
by shellcheck (Bash linter, static code analysis tool).

I even suspected there might be a typo, but it appeared correct on the
first pass. Man needs eagle eyes to spot typos for programming. That's
maybe even more important than being good at coming with algorithmic
solutions to abstract problems. A good solution is not worth much if
it doesn't work because of a typo.

* Wed 14 Jun 2023 11:10:31 AM CEST
Vault management command is displaying the 'sudo warning' multiple
times. It's ok to only show it if the sudo password hasn't been
recently entered. There is a way to check for cached sudo password
with a non interactive sudo command, (sudo -n), for example `sudo -n
true` and then testing the result.

Running `playtrough.sh 1 2` (to execute steps 1 and 2) works well,
whether user has exported a vault passphrase or types it in. Vault
creation and access test works, whether it had to be created, opened
or just accessed. Can commit changes.

